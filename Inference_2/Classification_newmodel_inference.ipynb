{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 64)      3520      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 512)         4719104   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 65,511,681\n",
      "Trainable params: 65,482,241\n",
      "Non-trainable params: 29,440\n",
      "_________________________________________________________________\n",
      "y_hat mean:0.197145 median:0.161941\n",
      "Sensitive TP/(TP+FN) : 0.550000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       100\n",
      "           1       0.55      0.55      0.55       100\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n",
      "y_hat mean:0.201568 median:0.158336\n",
      "Sensitive TP/(TP+FN) : 0.540000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54       100\n",
      "           1       0.54      0.54      0.54       100\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       200\n",
      "   macro avg       0.54      0.54      0.54       200\n",
      "weighted avg       0.54      0.54      0.54       200\n",
      "\n",
      "y_hat mean:0.183039 median:0.128965\n",
      "Sensitive TP/(TP+FN) : 0.550000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       100\n",
      "           1       0.55      0.55      0.55       100\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n",
      "y_hat mean:0.178795 median:0.137182\n",
      "Sensitive TP/(TP+FN) : 0.490000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49       100\n",
      "           1       0.49      0.49      0.49       100\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       200\n",
      "   macro avg       0.49      0.49      0.49       200\n",
      "weighted avg       0.49      0.49      0.49       200\n",
      "\n",
      "y_hat mean:0.181861 median:0.144300\n",
      "Sensitive TP/(TP+FN) : 0.570000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57       100\n",
      "           1       0.57      0.57      0.57       100\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       200\n",
      "   macro avg       0.57      0.57      0.57       200\n",
      "weighted avg       0.57      0.57      0.57       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.174402 median:0.138039\n",
      "Sensitive TP/(TP+FN) : 0.540000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54       100\n",
      "           1       0.54      0.54      0.54       100\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       200\n",
      "   macro avg       0.54      0.54      0.54       200\n",
      "weighted avg       0.54      0.54      0.54       200\n",
      "\n",
      "y_hat mean:0.174148 median:0.137136\n",
      "Sensitive TP/(TP+FN) : 0.560000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56       100\n",
      "           1       0.56      0.56      0.56       100\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       200\n",
      "   macro avg       0.56      0.56      0.56       200\n",
      "weighted avg       0.56      0.56      0.56       200\n",
      "\n",
      "y_hat mean:0.170343 median:0.138027\n",
      "Sensitive TP/(TP+FN) : 0.490000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49       100\n",
      "           1       0.49      0.49      0.49       100\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       200\n",
      "   macro avg       0.49      0.49      0.49       200\n",
      "weighted avg       0.49      0.49      0.49       200\n",
      "\n",
      "y_hat mean:0.172988 median:0.133648\n",
      "Sensitive TP/(TP+FN) : 0.470000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.47      0.47       100\n",
      "           1       0.47      0.47      0.47       100\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       200\n",
      "   macro avg       0.47      0.47      0.47       200\n",
      "weighted avg       0.47      0.47      0.47       200\n",
      "\n",
      "y_hat mean:0.179753 median:0.148765\n",
      "Sensitive TP/(TP+FN) : 0.500000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       100\n",
      "           1       0.50      0.50      0.50       100\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       0.50      0.50      0.50       200\n",
      "\n",
      "y_hat mean:0.183806 median:0.139908\n",
      "Sensitive TP/(TP+FN) : 0.520000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52       100\n",
      "           1       0.52      0.52      0.52       100\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       200\n",
      "   macro avg       0.52      0.52      0.52       200\n",
      "weighted avg       0.52      0.52      0.52       200\n",
      "\n",
      "y_hat mean:0.180473 median:0.141034\n",
      "Sensitive TP/(TP+FN) : 0.520000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52       100\n",
      "           1       0.52      0.52      0.52       100\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       200\n",
      "   macro avg       0.52      0.52      0.52       200\n",
      "weighted avg       0.52      0.52      0.52       200\n",
      "\n",
      "y_hat mean:0.195379 median:0.162108\n",
      "Sensitive TP/(TP+FN) : 0.500000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       100\n",
      "           1       0.50      0.50      0.50       100\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       0.50      0.50      0.50       200\n",
      "\n",
      "y_hat mean:0.175593 median:0.151158\n",
      "Sensitive TP/(TP+FN) : 0.520000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52       100\n",
      "           1       0.52      0.52      0.52       100\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       200\n",
      "   macro avg       0.52      0.52      0.52       200\n",
      "weighted avg       0.52      0.52      0.52       200\n",
      "\n",
      "y_hat mean:0.174485 median:0.132741\n",
      "Sensitive TP/(TP+FN) : 0.530000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.53      0.53       100\n",
      "           1       0.53      0.53      0.53       100\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       200\n",
      "   macro avg       0.53      0.53      0.53       200\n",
      "weighted avg       0.53      0.53      0.53       200\n",
      "\n",
      "y_hat mean:0.163620 median:0.137416\n",
      "Sensitive TP/(TP+FN) : 0.500000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       100\n",
      "           1       0.50      0.50      0.50       100\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       0.50      0.50      0.50       200\n",
      "\n",
      "y_hat mean:0.190323 median:0.154323\n",
      "Sensitive TP/(TP+FN) : 0.510000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51       100\n",
      "           1       0.51      0.51      0.51       100\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       200\n",
      "   macro avg       0.51      0.51      0.51       200\n",
      "weighted avg       0.51      0.51      0.51       200\n",
      "\n",
      "y_hat mean:0.178197 median:0.139420\n",
      "Sensitive TP/(TP+FN) : 0.480000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.48      0.48       100\n",
      "           1       0.48      0.48      0.48       100\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       200\n",
      "   macro avg       0.48      0.48      0.48       200\n",
      "weighted avg       0.48      0.48      0.48       200\n",
      "\n",
      "y_hat mean:0.180428 median:0.147364\n",
      "Sensitive TP/(TP+FN) : 0.530000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.53      0.53       100\n",
      "           1       0.53      0.53      0.53       100\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       200\n",
      "   macro avg       0.53      0.53      0.53       200\n",
      "weighted avg       0.53      0.53      0.53       200\n",
      "\n",
      "y_hat mean:0.171027 median:0.134483\n",
      "Sensitive TP/(TP+FN) : 0.620000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62       100\n",
      "           1       0.62      0.62      0.62       100\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       200\n",
      "   macro avg       0.62      0.62      0.62       200\n",
      "weighted avg       0.62      0.62      0.62       200\n",
      "\n",
      "y_hat mean:0.170676 median:0.141677\n",
      "Sensitive TP/(TP+FN) : 0.530000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.53      0.53       100\n",
      "           1       0.53      0.53      0.53       100\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       200\n",
      "   macro avg       0.53      0.53      0.53       200\n",
      "weighted avg       0.53      0.53      0.53       200\n",
      "\n",
      "y_hat mean:0.203557 median:0.166633\n",
      "Sensitive TP/(TP+FN) : 0.550000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       100\n",
      "           1       0.55      0.55      0.55       100\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n",
      "y_hat mean:0.166143 median:0.132959\n",
      "Sensitive TP/(TP+FN) : 0.550000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       100\n",
      "           1       0.55      0.55      0.55       100\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n",
      "y_hat mean:0.180009 median:0.147715\n",
      "Sensitive TP/(TP+FN) : 0.500000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       100\n",
      "           1       0.50      0.50      0.50       100\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       0.50      0.50      0.50       200\n",
      "\n",
      "y_hat mean:0.178452 median:0.135527\n",
      "Sensitive TP/(TP+FN) : 0.550000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       100\n",
      "           1       0.55      0.55      0.55       100\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n",
      "y_hat mean:0.163772 median:0.129776\n",
      "Sensitive TP/(TP+FN) : 0.500000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       100\n",
      "           1       0.50      0.50      0.50       100\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       0.50      0.50      0.50       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.167648 median:0.131580\n",
      "Sensitive TP/(TP+FN) : 0.560000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56       100\n",
      "           1       0.56      0.56      0.56       100\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       200\n",
      "   macro avg       0.56      0.56      0.56       200\n",
      "weighted avg       0.56      0.56      0.56       200\n",
      "\n",
      "y_hat mean:0.186546 median:0.145000\n",
      "Sensitive TP/(TP+FN) : 0.510000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51       100\n",
      "           1       0.51      0.51      0.51       100\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       200\n",
      "   macro avg       0.51      0.51      0.51       200\n",
      "weighted avg       0.51      0.51      0.51       200\n",
      "\n",
      "y_hat mean:0.164846 median:0.133387\n",
      "Sensitive TP/(TP+FN) : 0.530000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.53      0.53       100\n",
      "           1       0.53      0.53      0.53       100\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       200\n",
      "   macro avg       0.53      0.53      0.53       200\n",
      "weighted avg       0.53      0.53      0.53       200\n",
      "\n",
      "y_hat mean:0.170374 median:0.143303\n",
      "Sensitive TP/(TP+FN) : 0.580000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.58      0.58       100\n",
      "           1       0.58      0.58      0.58       100\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       200\n",
      "   macro avg       0.58      0.58      0.58       200\n",
      "weighted avg       0.58      0.58      0.58       200\n",
      "\n",
      "y_hat mean:0.182059 median:0.148194\n",
      "Sensitive TP/(TP+FN) : 0.500000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       100\n",
      "           1       0.50      0.50      0.50       100\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       0.50      0.50      0.50       200\n",
      "\n",
      "y_hat mean:0.162157 median:0.139853\n",
      "Sensitive TP/(TP+FN) : 0.540000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54       100\n",
      "           1       0.54      0.54      0.54       100\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       200\n",
      "   macro avg       0.54      0.54      0.54       200\n",
      "weighted avg       0.54      0.54      0.54       200\n",
      "\n",
      "y_hat mean:0.181259 median:0.145760\n",
      "Sensitive TP/(TP+FN) : 0.550000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       100\n",
      "           1       0.55      0.55      0.55       100\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n",
      "y_hat mean:0.169647 median:0.134512\n",
      "Sensitive TP/(TP+FN) : 0.560000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56       100\n",
      "           1       0.56      0.56      0.56       100\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       200\n",
      "   macro avg       0.56      0.56      0.56       200\n",
      "weighted avg       0.56      0.56      0.56       200\n",
      "\n",
      "y_hat mean:0.182781 median:0.141704\n",
      "Sensitive TP/(TP+FN) : 0.550000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       100\n",
      "           1       0.55      0.55      0.55       100\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n",
      "y_hat mean:0.176307 median:0.136522\n",
      "Sensitive TP/(TP+FN) : 0.530000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.53      0.53       100\n",
      "           1       0.53      0.53      0.53       100\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       200\n",
      "   macro avg       0.53      0.53      0.53       200\n",
      "weighted avg       0.53      0.53      0.53       200\n",
      "\n",
      "y_hat mean:0.177490 median:0.148230\n",
      "Sensitive TP/(TP+FN) : 0.540000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54       100\n",
      "           1       0.54      0.54      0.54       100\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       200\n",
      "   macro avg       0.54      0.54      0.54       200\n",
      "weighted avg       0.54      0.54      0.54       200\n",
      "\n",
      "y_hat mean:0.172362 median:0.134667\n",
      "Sensitive TP/(TP+FN) : 0.430000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.43      0.43       100\n",
      "           1       0.43      0.43      0.43       100\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       200\n",
      "   macro avg       0.43      0.43      0.43       200\n",
      "weighted avg       0.43      0.43      0.43       200\n",
      "\n",
      "y_hat mean:0.183856 median:0.147633\n",
      "Sensitive TP/(TP+FN) : 0.530000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.53      0.53       100\n",
      "           1       0.53      0.53      0.53       100\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       200\n",
      "   macro avg       0.53      0.53      0.53       200\n",
      "weighted avg       0.53      0.53      0.53       200\n",
      "\n",
      "y_hat mean:0.171425 median:0.134203\n",
      "Sensitive TP/(TP+FN) : 0.500000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       100\n",
      "           1       0.50      0.50      0.50       100\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       0.50      0.50      0.50       200\n",
      "\n",
      "y_hat mean:0.185592 median:0.142429\n",
      "Sensitive TP/(TP+FN) : 0.490000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49       100\n",
      "           1       0.49      0.49      0.49       100\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       200\n",
      "   macro avg       0.49      0.49      0.49       200\n",
      "weighted avg       0.49      0.49      0.49       200\n",
      "\n",
      "y_hat mean:0.176775 median:0.149633\n",
      "Sensitive TP/(TP+FN) : 0.560000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56       100\n",
      "           1       0.56      0.56      0.56       100\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       200\n",
      "   macro avg       0.56      0.56      0.56       200\n",
      "weighted avg       0.56      0.56      0.56       200\n",
      "\n",
      "y_hat mean:0.184581 median:0.150700\n",
      "Sensitive TP/(TP+FN) : 0.490000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49       100\n",
      "           1       0.49      0.49      0.49       100\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       200\n",
      "   macro avg       0.49      0.49      0.49       200\n",
      "weighted avg       0.49      0.49      0.49       200\n",
      "\n",
      "y_hat mean:0.175196 median:0.142107\n",
      "Sensitive TP/(TP+FN) : 0.500000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       100\n",
      "           1       0.50      0.50      0.50       100\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       0.50      0.50      0.50       200\n",
      "\n",
      "y_hat mean:0.175744 median:0.157395\n",
      "Sensitive TP/(TP+FN) : 0.560000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56       100\n",
      "           1       0.56      0.56      0.56       100\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       200\n",
      "   macro avg       0.56      0.56      0.56       200\n",
      "weighted avg       0.56      0.56      0.56       200\n",
      "\n",
      "y_hat mean:0.176019 median:0.139414\n",
      "Sensitive TP/(TP+FN) : 0.540000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54       100\n",
      "           1       0.54      0.54      0.54       100\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       200\n",
      "   macro avg       0.54      0.54      0.54       200\n",
      "weighted avg       0.54      0.54      0.54       200\n",
      "\n",
      "y_hat mean:0.176246 median:0.144761\n",
      "Sensitive TP/(TP+FN) : 0.510000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51       100\n",
      "           1       0.51      0.51      0.51       100\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       200\n",
      "   macro avg       0.51      0.51      0.51       200\n",
      "weighted avg       0.51      0.51      0.51       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.166823 median:0.142079\n",
      "Sensitive TP/(TP+FN) : 0.520000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52       100\n",
      "           1       0.52      0.52      0.52       100\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       200\n",
      "   macro avg       0.52      0.52      0.52       200\n",
      "weighted avg       0.52      0.52      0.52       200\n",
      "\n",
      "y_hat mean:0.173483 median:0.141300\n",
      "Sensitive TP/(TP+FN) : 0.440000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.44      0.44       100\n",
      "           1       0.44      0.44      0.44       100\n",
      "\n",
      "   micro avg       0.44      0.44      0.44       200\n",
      "   macro avg       0.44      0.44      0.44       200\n",
      "weighted avg       0.44      0.44      0.44       200\n",
      "\n",
      "y_hat mean:0.180368 median:0.145889\n",
      "Sensitive TP/(TP+FN) : 0.550000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       100\n",
      "           1       0.55      0.55      0.55       100\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.55       200\n",
      "\n",
      "y_hat mean:0.174228 median:0.136594\n",
      "Sensitive TP/(TP+FN) : 0.510000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51       100\n",
      "           1       0.51      0.51      0.51       100\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       200\n",
      "   macro avg       0.51      0.51      0.51       200\n",
      "weighted avg       0.51      0.51      0.51       200\n",
      "\n",
      "(10200, 1)\n",
      "(10200, 1)\n",
      "y_hat mean:0.500000 median:0.500000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from plotLayer import *\n",
    "from preprocess import *\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy import sparse as sp\n",
    "from skimage.measure import block_reduce\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report, precision_recall_curve\n",
    "\n",
    "from model import *\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "def Classify_Rate(y, y_hat):\n",
    "    # True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
    "    TP = np.sum(np.logical_and(y_hat == 1, y == 1))\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(y_hat == 0, y == 0))\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(y_hat == 1, y == 0))\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(y_hat == 0, y == 1))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def Predict(model, x, y_threshold=None):\n",
    "    y_hat = model.predict(x)\n",
    "\n",
    "    if y_threshold :\n",
    "        y_hat[y_hat < y_threshold] = 0\n",
    "        y_hat[y_hat >= y_threshold] = 1\n",
    "    return y_hat\n",
    "\n",
    "def PlotROC(y, y_hat):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(2):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y, y_hat)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    print (\"roc_auc_score:%f\" %roc_auc_score(y, y_hat))\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[1], tpr[1])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.show()\n",
    "\n",
    "def PreprocessData(datapath, width=256, channel=6):\n",
    "    ratio = 1024 // width\n",
    "    with open(datapath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    x = np.heaviside(np.array([map(lambda x: block_reduce(x.toarray(), block_size=(ratio,ratio), func=np.max), d.hL) for d in data]), 0)\n",
    "    x = np.swapaxes(x, 1, 3)\n",
    "    y = np.array([d.label for d in data])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    width = 256\n",
    "    channel = 6\n",
    "    #classify_weights_path = \"Classify_epoch_50_batch_4.hdf5\" the below model is copied one directory above\n",
    "    classify_weights_path = \"Classify_epoch_50_batch_5.hdf5\"\n",
    "\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "    classify_model = Encoder_Classify(input_size=(width,width,6), batch_normal=True)\n",
    "    classify_model.load_weights(classify_weights_path)\n",
    "\n",
    "    d0_path = \"../Data/1stDataset/d0*\"\n",
    "    false_path = \"../Data/1stDataset/false*\"\n",
    "    d0 = np.sort(glob.glob(d0_path))\n",
    "    f0 = np.sort(glob.glob(false_path))\n",
    "\n",
    "    y_total = []\n",
    "    y_hat_total = []\n",
    "    #proba is a keyword used in sklearn for probabilities\n",
    "    proba = []\n",
    "#     301\n",
    "    for i in range(500,551):\n",
    "        x1, y1 = PreprocessData(d0[i], width=width, channel=channel)\n",
    "        x2, y2 = PreprocessData(f0[i], width=width, channel=channel)\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "        y = np.concatenate((y1, y2), axis=0)\n",
    "\n",
    "        y_hat = Predict(classify_model, x)\n",
    "#         print(y_hat)\n",
    "        y_total += [y]\n",
    "        y_hat_total += [y_hat]\n",
    "        proba += [y_hat.copy()]\n",
    "# print(proba)-------You should be very careful in numpy as numpy arrays as get cop\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html\n",
    "# Check the section sub arrays as no copy views\n",
    "        print(\"y_hat mean:%f median:%f\" %(np.mean(y_hat), np.median(y_hat)))\n",
    "        y_hat[y_hat < np.median(y_hat)] = 0\n",
    "        y_hat[y_hat >= np.median(y_hat)] = 1\n",
    "\n",
    "        TP, TN, FP, FN = Classify_Rate(y, y_hat)\n",
    "        print(\"Sensitive TP/(TP+FN) : %f\" %(TP / (TP + FN)))\n",
    "        print(classification_report(y, y_hat))\n",
    "\n",
    "    y = np.concatenate(y_total)\n",
    "    y_hat = np.concatenate(y_hat_total)\n",
    "    proba = np.concatenate(proba)\n",
    "    a=np.hstack((y,proba))\n",
    "    np.savetxt(\"newmodel_y_y_hat.txt\",a,delimiter=',')\n",
    "    print(y.shape)\n",
    "    print(y_hat.shape)\n",
    "    print(\"y_hat mean:%f median:%f\" %(np.mean(y_hat), np.median(y_hat)))\n",
    "    fp,tp,tr=roc_curve(y,proba)\n",
    "    p,r,tr1=precision_recall_curve(y,proba)\n",
    "    plt.figure(1)\n",
    "    plt.plot(fp,tp,color='darkorange',lw=2,label='ROC curve(area = %0.3f)'%auc(fp,tp))\n",
    "    plt.xlabel('False positive Rate')\n",
    "    plt.ylabel('True positive Rate')\n",
    "    legend = plt.legend(fontsize = 'x-large')\n",
    "    plt.savefig('ROC_curve_newmodel.png')\n",
    "    plt.show()\n",
    "    plt.figure(2)\n",
    "    plt.plot(r,p,color='darkorange', label='Precision recall curve') \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    legend=plt.legend(fontsize='x-large')\n",
    "    plt.savefig('Precision_Recall_newmodel.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve area for 10 samples =0.538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive TP/(TP+FN) : 0.524510\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52      5100\n",
      "           1       0.52      0.52      0.52      5100\n",
      "\n",
      "   micro avg       0.52      0.52      0.52     10200\n",
      "   macro avg       0.52      0.52      0.52     10200\n",
      "weighted avg       0.52      0.52      0.52     10200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    y_hat[y_hat < np.median(y_hat)] = 0\n",
    "    y_hat[y_hat >= np.median(y_hat)] = 1\n",
    "    TP, TN, FP, FN = Classify_Rate(y, y_hat)\n",
    "    print(\"Sensitive TP/(TP+FN) : %f\" %(TP / (TP + FN)))\n",
    "    print(classification_report(y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(proba)\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [0.8, 0.9, 0.85, 0]).ravel()\n",
    "# What ever I have presented yesterday those contain thresholded values. \n",
    "# from sklearn.metrics import classification_report\n",
    "# y_true=np.array([0,1,1,1,0,0,1])\n",
    "# y_pred=np.array([0.3,0.9,0.8,0.3,0.2,0.3,0.8])\n",
    "# print(classification_report(y_true, y_pred))\n",
    "#Classification report as well as the confusion_matrix works only for a particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
