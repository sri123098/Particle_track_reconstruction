{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 64)      3520      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 512)         4719104   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 65,511,681\n",
      "Trainable params: 65,482,241\n",
      "Non-trainable params: 29,440\n",
      "_________________________________________________________________\n",
      "y_hat mean:0.333876 median:0.325603\n",
      "Sensitive TP/(TP+FN) : 0.460000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.46      0.46       100\n",
      "           1       0.46      0.46      0.46       100\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       200\n",
      "   macro avg       0.46      0.46      0.46       200\n",
      "weighted avg       0.46      0.46      0.46       200\n",
      "\n",
      "y_hat mean:0.309394 median:0.291288\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.339834 median:0.338900\n",
      "Sensitive TP/(TP+FN) : 0.420000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42       100\n",
      "           1       0.42      0.42      0.42       100\n",
      "\n",
      "   micro avg       0.42      0.42      0.42       200\n",
      "   macro avg       0.42      0.42      0.42       200\n",
      "weighted avg       0.42      0.42      0.42       200\n",
      "\n",
      "y_hat mean:0.326591 median:0.309469\n",
      "Sensitive TP/(TP+FN) : 0.370000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37       100\n",
      "           1       0.37      0.37      0.37       100\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       200\n",
      "   macro avg       0.37      0.37      0.37       200\n",
      "weighted avg       0.37      0.37      0.37       200\n",
      "\n",
      "y_hat mean:0.314632 median:0.317548\n",
      "Sensitive TP/(TP+FN) : 0.430000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.43      0.43       100\n",
      "           1       0.43      0.43      0.43       100\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       200\n",
      "   macro avg       0.43      0.43      0.43       200\n",
      "weighted avg       0.43      0.43      0.43       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.346977 median:0.363431\n",
      "Sensitive TP/(TP+FN) : 0.420000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42       100\n",
      "           1       0.42      0.42      0.42       100\n",
      "\n",
      "   micro avg       0.42      0.42      0.42       200\n",
      "   macro avg       0.42      0.42      0.42       200\n",
      "weighted avg       0.42      0.42      0.42       200\n",
      "\n",
      "y_hat mean:0.322503 median:0.307645\n",
      "Sensitive TP/(TP+FN) : 0.390000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.39      0.39       100\n",
      "           1       0.39      0.39      0.39       100\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       200\n",
      "   macro avg       0.39      0.39      0.39       200\n",
      "weighted avg       0.39      0.39      0.39       200\n",
      "\n",
      "y_hat mean:0.344677 median:0.335938\n",
      "Sensitive TP/(TP+FN) : 0.300000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.30      0.30       100\n",
      "           1       0.30      0.30      0.30       100\n",
      "\n",
      "   micro avg       0.30      0.30      0.30       200\n",
      "   macro avg       0.30      0.30      0.30       200\n",
      "weighted avg       0.30      0.30      0.30       200\n",
      "\n",
      "y_hat mean:0.333619 median:0.322353\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.312235 median:0.295540\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.325889 median:0.321598\n",
      "Sensitive TP/(TP+FN) : 0.370000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37       100\n",
      "           1       0.37      0.37      0.37       100\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       200\n",
      "   macro avg       0.37      0.37      0.37       200\n",
      "weighted avg       0.37      0.37      0.37       200\n",
      "\n",
      "y_hat mean:0.312796 median:0.318099\n",
      "Sensitive TP/(TP+FN) : 0.400000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.40      0.40       100\n",
      "           1       0.40      0.40      0.40       100\n",
      "\n",
      "   micro avg       0.40      0.40      0.40       200\n",
      "   macro avg       0.40      0.40      0.40       200\n",
      "weighted avg       0.40      0.40      0.40       200\n",
      "\n",
      "y_hat mean:0.336715 median:0.344323\n",
      "Sensitive TP/(TP+FN) : 0.390000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.39      0.39       100\n",
      "           1       0.39      0.39      0.39       100\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       200\n",
      "   macro avg       0.39      0.39      0.39       200\n",
      "weighted avg       0.39      0.39      0.39       200\n",
      "\n",
      "y_hat mean:0.345034 median:0.351859\n",
      "Sensitive TP/(TP+FN) : 0.370000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37       100\n",
      "           1       0.37      0.37      0.37       100\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       200\n",
      "   macro avg       0.37      0.37      0.37       200\n",
      "weighted avg       0.37      0.37      0.37       200\n",
      "\n",
      "y_hat mean:0.327649 median:0.313996\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.320455 median:0.288354\n",
      "Sensitive TP/(TP+FN) : 0.360000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.36      0.36       100\n",
      "           1       0.36      0.36      0.36       100\n",
      "\n",
      "   micro avg       0.36      0.36      0.36       200\n",
      "   macro avg       0.36      0.36      0.36       200\n",
      "weighted avg       0.36      0.36      0.36       200\n",
      "\n",
      "y_hat mean:0.340228 median:0.335216\n",
      "Sensitive TP/(TP+FN) : 0.370000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37       100\n",
      "           1       0.37      0.37      0.37       100\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       200\n",
      "   macro avg       0.37      0.37      0.37       200\n",
      "weighted avg       0.37      0.37      0.37       200\n",
      "\n",
      "y_hat mean:0.338673 median:0.356297\n",
      "Sensitive TP/(TP+FN) : 0.360000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.36      0.36       100\n",
      "           1       0.36      0.36      0.36       100\n",
      "\n",
      "   micro avg       0.36      0.36      0.36       200\n",
      "   macro avg       0.36      0.36      0.36       200\n",
      "weighted avg       0.36      0.36      0.36       200\n",
      "\n",
      "y_hat mean:0.330397 median:0.344106\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.332274 median:0.322139\n",
      "Sensitive TP/(TP+FN) : 0.360000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.36      0.36       100\n",
      "           1       0.36      0.36      0.36       100\n",
      "\n",
      "   micro avg       0.36      0.36      0.36       200\n",
      "   macro avg       0.36      0.36      0.36       200\n",
      "weighted avg       0.36      0.36      0.36       200\n",
      "\n",
      "y_hat mean:0.313381 median:0.286192\n",
      "Sensitive TP/(TP+FN) : 0.370000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37       100\n",
      "           1       0.37      0.37      0.37       100\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       200\n",
      "   macro avg       0.37      0.37      0.37       200\n",
      "weighted avg       0.37      0.37      0.37       200\n",
      "\n",
      "y_hat mean:0.360004 median:0.367609\n",
      "Sensitive TP/(TP+FN) : 0.340000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.34      0.34       100\n",
      "           1       0.34      0.34      0.34       100\n",
      "\n",
      "   micro avg       0.34      0.34      0.34       200\n",
      "   macro avg       0.34      0.34      0.34       200\n",
      "weighted avg       0.34      0.34      0.34       200\n",
      "\n",
      "y_hat mean:0.330972 median:0.344348\n",
      "Sensitive TP/(TP+FN) : 0.360000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.36      0.36       100\n",
      "           1       0.36      0.36      0.36       100\n",
      "\n",
      "   micro avg       0.36      0.36      0.36       200\n",
      "   macro avg       0.36      0.36      0.36       200\n",
      "weighted avg       0.36      0.36      0.36       200\n",
      "\n",
      "y_hat mean:0.333986 median:0.300487\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.359594 median:0.346481\n",
      "Sensitive TP/(TP+FN) : 0.410000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.41      0.41       100\n",
      "           1       0.41      0.41      0.41       100\n",
      "\n",
      "   micro avg       0.41      0.41      0.41       200\n",
      "   macro avg       0.41      0.41      0.41       200\n",
      "weighted avg       0.41      0.41      0.41       200\n",
      "\n",
      "y_hat mean:0.335829 median:0.313679\n",
      "Sensitive TP/(TP+FN) : 0.350000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35       100\n",
      "           1       0.35      0.35      0.35       100\n",
      "\n",
      "   micro avg       0.35      0.35      0.35       200\n",
      "   macro avg       0.35      0.35      0.35       200\n",
      "weighted avg       0.35      0.35      0.35       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.332918 median:0.322616\n",
      "Sensitive TP/(TP+FN) : 0.420000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42       100\n",
      "           1       0.42      0.42      0.42       100\n",
      "\n",
      "   micro avg       0.42      0.42      0.42       200\n",
      "   macro avg       0.42      0.42      0.42       200\n",
      "weighted avg       0.42      0.42      0.42       200\n",
      "\n",
      "y_hat mean:0.349918 median:0.369728\n",
      "Sensitive TP/(TP+FN) : 0.440000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.44      0.44       100\n",
      "           1       0.44      0.44      0.44       100\n",
      "\n",
      "   micro avg       0.44      0.44      0.44       200\n",
      "   macro avg       0.44      0.44      0.44       200\n",
      "weighted avg       0.44      0.44      0.44       200\n",
      "\n",
      "y_hat mean:0.336657 median:0.334946\n",
      "Sensitive TP/(TP+FN) : 0.390000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.39      0.39       100\n",
      "           1       0.39      0.39      0.39       100\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       200\n",
      "   macro avg       0.39      0.39      0.39       200\n",
      "weighted avg       0.39      0.39      0.39       200\n",
      "\n",
      "y_hat mean:0.320553 median:0.321957\n",
      "Sensitive TP/(TP+FN) : 0.410000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.41      0.41       100\n",
      "           1       0.41      0.41      0.41       100\n",
      "\n",
      "   micro avg       0.41      0.41      0.41       200\n",
      "   macro avg       0.41      0.41      0.41       200\n",
      "weighted avg       0.41      0.41      0.41       200\n",
      "\n",
      "y_hat mean:0.322181 median:0.302064\n",
      "Sensitive TP/(TP+FN) : 0.376238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37        99\n",
      "           1       0.38      0.38      0.38       101\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.37      0.37       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.328558 median:0.332816\n",
      "Sensitive TP/(TP+FN) : 0.330000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.33      0.33       100\n",
      "           1       0.33      0.33      0.33       100\n",
      "\n",
      "   micro avg       0.33      0.33      0.33       200\n",
      "   macro avg       0.33      0.33      0.33       200\n",
      "weighted avg       0.33      0.33      0.33       200\n",
      "\n",
      "y_hat mean:0.339305 median:0.329036\n",
      "Sensitive TP/(TP+FN) : 0.390000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.39      0.39       100\n",
      "           1       0.39      0.39      0.39       100\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       200\n",
      "   macro avg       0.39      0.39      0.39       200\n",
      "weighted avg       0.39      0.39      0.39       200\n",
      "\n",
      "y_hat mean:0.330560 median:0.333579\n",
      "Sensitive TP/(TP+FN) : 0.410000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.41      0.41       100\n",
      "           1       0.41      0.41      0.41       100\n",
      "\n",
      "   micro avg       0.41      0.41      0.41       200\n",
      "   macro avg       0.41      0.41      0.41       200\n",
      "weighted avg       0.41      0.41      0.41       200\n",
      "\n",
      "y_hat mean:0.352244 median:0.368470\n",
      "Sensitive TP/(TP+FN) : 0.450000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.45      0.45       100\n",
      "           1       0.45      0.45      0.45       100\n",
      "\n",
      "   micro avg       0.45      0.45      0.45       200\n",
      "   macro avg       0.45      0.45      0.45       200\n",
      "weighted avg       0.45      0.45      0.45       200\n",
      "\n",
      "y_hat mean:0.331238 median:0.321697\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.333203 median:0.320526\n",
      "Sensitive TP/(TP+FN) : 0.350000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35       100\n",
      "           1       0.35      0.35      0.35       100\n",
      "\n",
      "   micro avg       0.35      0.35      0.35       200\n",
      "   macro avg       0.35      0.35      0.35       200\n",
      "weighted avg       0.35      0.35      0.35       200\n",
      "\n",
      "y_hat mean:0.323932 median:0.305451\n",
      "Sensitive TP/(TP+FN) : 0.300000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.30      0.30       100\n",
      "           1       0.30      0.30      0.30       100\n",
      "\n",
      "   micro avg       0.30      0.30      0.30       200\n",
      "   macro avg       0.30      0.30      0.30       200\n",
      "weighted avg       0.30      0.30      0.30       200\n",
      "\n",
      "y_hat mean:0.345004 median:0.338864\n",
      "Sensitive TP/(TP+FN) : 0.330000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.33      0.33       100\n",
      "           1       0.33      0.33      0.33       100\n",
      "\n",
      "   micro avg       0.33      0.33      0.33       200\n",
      "   macro avg       0.33      0.33      0.33       200\n",
      "weighted avg       0.33      0.33      0.33       200\n",
      "\n",
      "y_hat mean:0.319916 median:0.295836\n",
      "Sensitive TP/(TP+FN) : 0.370000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37       100\n",
      "           1       0.37      0.37      0.37       100\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       200\n",
      "   macro avg       0.37      0.37      0.37       200\n",
      "weighted avg       0.37      0.37      0.37       200\n",
      "\n",
      "y_hat mean:0.330958 median:0.344525\n",
      "Sensitive TP/(TP+FN) : 0.440000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.44      0.44       100\n",
      "           1       0.44      0.44      0.44       100\n",
      "\n",
      "   micro avg       0.44      0.44      0.44       200\n",
      "   macro avg       0.44      0.44      0.44       200\n",
      "weighted avg       0.44      0.44      0.44       200\n",
      "\n",
      "y_hat mean:0.330909 median:0.327197\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.349868 median:0.357213\n",
      "Sensitive TP/(TP+FN) : 0.380000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       100\n",
      "           1       0.38      0.38      0.38       100\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       200\n",
      "   macro avg       0.38      0.38      0.38       200\n",
      "weighted avg       0.38      0.38      0.38       200\n",
      "\n",
      "y_hat mean:0.327664 median:0.326826\n",
      "Sensitive TP/(TP+FN) : 0.400000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.40      0.40       100\n",
      "           1       0.40      0.40      0.40       100\n",
      "\n",
      "   micro avg       0.40      0.40      0.40       200\n",
      "   macro avg       0.40      0.40      0.40       200\n",
      "weighted avg       0.40      0.40      0.40       200\n",
      "\n",
      "y_hat mean:0.328683 median:0.319703\n",
      "Sensitive TP/(TP+FN) : 0.460000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.46      0.46       100\n",
      "           1       0.46      0.46      0.46       100\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       200\n",
      "   macro avg       0.46      0.46      0.46       200\n",
      "weighted avg       0.46      0.46      0.46       200\n",
      "\n",
      "y_hat mean:0.338025 median:0.341436\n",
      "Sensitive TP/(TP+FN) : 0.350000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35       100\n",
      "           1       0.35      0.35      0.35       100\n",
      "\n",
      "   micro avg       0.35      0.35      0.35       200\n",
      "   macro avg       0.35      0.35      0.35       200\n",
      "weighted avg       0.35      0.35      0.35       200\n",
      "\n",
      "y_hat mean:0.323626 median:0.330079\n",
      "Sensitive TP/(TP+FN) : 0.300000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.30      0.30       100\n",
      "           1       0.30      0.30      0.30       100\n",
      "\n",
      "   micro avg       0.30      0.30      0.30       200\n",
      "   macro avg       0.30      0.30      0.30       200\n",
      "weighted avg       0.30      0.30      0.30       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.351061 median:0.350379\n",
      "Sensitive TP/(TP+FN) : 0.430000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.43      0.43       100\n",
      "           1       0.43      0.43      0.43       100\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       200\n",
      "   macro avg       0.43      0.43      0.43       200\n",
      "weighted avg       0.43      0.43      0.43       200\n",
      "\n",
      "y_hat mean:0.310189 median:0.276437\n",
      "Sensitive TP/(TP+FN) : 0.370000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37       100\n",
      "           1       0.37      0.37      0.37       100\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       200\n",
      "   macro avg       0.37      0.37      0.37       200\n",
      "weighted avg       0.37      0.37      0.37       200\n",
      "\n",
      "y_hat mean:0.323179 median:0.318096\n",
      "Sensitive TP/(TP+FN) : 0.490000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49       100\n",
      "           1       0.49      0.49      0.49       100\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       200\n",
      "   macro avg       0.49      0.49      0.49       200\n",
      "weighted avg       0.49      0.49      0.49       200\n",
      "\n",
      "y_hat mean:0.326784 median:0.325400\n",
      "Sensitive TP/(TP+FN) : 0.410000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.41      0.41       100\n",
      "           1       0.41      0.41      0.41       100\n",
      "\n",
      "   micro avg       0.41      0.41      0.41       200\n",
      "   macro avg       0.41      0.41      0.41       200\n",
      "weighted avg       0.41      0.41      0.41       200\n",
      "\n",
      "(10200, 1)\n",
      "(10200, 1)\n",
      "y_hat mean:0.500000 median:0.500000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive TP/(TP+FN) : 0.385023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38      5099\n",
      "           1       0.39      0.39      0.39      5101\n",
      "\n",
      "   micro avg       0.39      0.39      0.38     10200\n",
      "   macro avg       0.39      0.38      0.38     10200\n",
      "weighted avg       0.39      0.39      0.39     10200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from plotLayer import *\n",
    "from preprocess import *\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy import sparse as sp\n",
    "from skimage.measure import block_reduce\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report, precision_recall_curve\n",
    "\n",
    "from model import *\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "def Classify_Rate(y, y_hat):\n",
    "    # True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
    "    TP = np.sum(np.logical_and(y_hat == 1, y == 1))\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(y_hat == 0, y == 0))\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(y_hat == 1, y == 0))\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(y_hat == 0, y == 1))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def Predict(model, x, y_threshold=None):\n",
    "    y_hat = model.predict(x)\n",
    "\n",
    "    if y_threshold :\n",
    "        y_hat[y_hat < y_threshold] = 0\n",
    "        y_hat[y_hat >= y_threshold] = 1\n",
    "    return y_hat\n",
    "\n",
    "def PlotROC(y, y_hat):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(2):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y, y_hat)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    print (\"roc_auc_score:%f\" %roc_auc_score(y, y_hat))\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[1], tpr[1])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.show()\n",
    "\n",
    "def PreprocessData(datapath, width=256, channel=6):\n",
    "    ratio = 1024 // width\n",
    "    with open(datapath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    x = np.heaviside(np.array([map(lambda x: block_reduce(x.toarray(), block_size=(ratio,ratio), func=np.max), d.hL) for d in data]), 0)\n",
    "    x = np.swapaxes(x, 1, 3)\n",
    "    y = np.array([d.label for d in data])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    width = 256\n",
    "    channel = 6\n",
    "    #classify_weights_path = \"Classify_epoch_50_batch_4.hdf5\"\n",
    "    classify_weights_path = \"Classify_epoch_100_batch_5.hdf5\"\n",
    "\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "    classify_model = Encoder_Classify(input_size=(width,width,6), batch_normal=True)\n",
    "    classify_model.load_weights(classify_weights_path)\n",
    "\n",
    "    d0_path = \"../Data/1stDataset/d0*\"\n",
    "    false_path = \"../Data/1stDataset/false*\"\n",
    "    d0 = np.sort(glob.glob(d0_path))\n",
    "    f0 = np.sort(glob.glob(false_path))\n",
    "\n",
    "    y_total = []\n",
    "    y_hat_total = []\n",
    "#     301\n",
    "    for i in range(250,301):\n",
    "        x1, y1 = PreprocessData(d0[i], width=width, channel=channel)\n",
    "        x2, y2 = PreprocessData(f0[i], width=width, channel=channel)\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "        y = np.concatenate((y1, y2), axis=0)\n",
    "\n",
    "        y_hat = Predict(classify_model, x)\n",
    "        y_total += [y]\n",
    "        y_hat_total += [y_hat]\n",
    "        print(\"y_hat mean:%f median:%f\" %(np.mean(y_hat), np.median(y_hat)))\n",
    "        y_hat[y_hat < np.median(y_hat)] = 0\n",
    "        y_hat[y_hat >= np.median(y_hat)] = 1\n",
    "\n",
    "        TP, TN, FP, FN = Classify_Rate(y, y_hat)\n",
    "        print(\"Sensitive TP/(TP+FN) : %f\" %(TP / (TP + FN)))\n",
    "        print(classification_report(y, y_hat))\n",
    "\n",
    "    y = np.concatenate(y_total)\n",
    "    y_hat = np.concatenate(y_hat_total)\n",
    "    print(y.shape)\n",
    "    print(y_hat.shape)\n",
    "    a=np.hstack((y,y_hat))\n",
    "    np.savetxt('values_250_301.txt', a, delimiter =',')\n",
    "    print(\"y_hat mean:%f median:%f\" %(np.mean(y_hat), np.median(y_hat)))\n",
    "    fp,tp,tr=roc_curve(y,y_hat)\n",
    "    p,r,tr1=precision_recall_curve(y,y_hat)\n",
    "    plt.plot(fp,tp,color='darkorange',lw=2,label='ROC curve(area = %0.3f)'%auc(fp,tp))\n",
    "    plt.xlabel('False positive Rate')\n",
    "    plt.ylabel('True positive Rate')\n",
    "    legend = plt.legend(fontsize = 'x-large')\n",
    "#     plt.savefig('ROC_curve.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot(r,p,color='darkorange', label='Precision recall curve') \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    legend=plt.legend(fontsize='x-large')\n",
    "#     plt.savefig('Precision_Recall.png')\n",
    "    plt.show()\n",
    "    y_hat[y_hat < np.median(y_hat)] = 0\n",
    "    y_hat[y_hat >= np.median(y_hat)] = 1\n",
    "    TP, TN, FP, FN = Classify_Rate(y, y_hat)\n",
    "    print(\"Sensitive TP/(TP+FN) : %f\" %(TP / (TP + FN)))\n",
    "    print(classification_report(y, y_hat))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve area = 0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
