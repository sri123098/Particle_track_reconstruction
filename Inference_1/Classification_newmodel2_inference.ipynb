{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 64)      3520      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 512)         4719104   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 65,511,681\n",
      "Trainable params: 65,482,241\n",
      "Non-trainable params: 29,440\n",
      "_________________________________________________________________\n",
      "y_hat mean:0.379732 median:0.334710\n",
      "Sensitive TP/(TP+FN) : 0.630000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63       100\n",
      "           1       0.63      0.63      0.63       100\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       200\n",
      "   macro avg       0.63      0.63      0.63       200\n",
      "weighted avg       0.63      0.63      0.63       200\n",
      "\n",
      "y_hat mean:0.348770 median:0.282034\n",
      "Sensitive TP/(TP+FN) : 0.610000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.61      0.61       100\n",
      "           1       0.61      0.61      0.61       100\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       200\n",
      "   macro avg       0.61      0.61      0.61       200\n",
      "weighted avg       0.61      0.61      0.61       200\n",
      "\n",
      "y_hat mean:0.308404 median:0.168418\n",
      "Sensitive TP/(TP+FN) : 0.640000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64       100\n",
      "           1       0.64      0.64      0.64       100\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       200\n",
      "   macro avg       0.64      0.64      0.64       200\n",
      "weighted avg       0.64      0.64      0.64       200\n",
      "\n",
      "y_hat mean:0.357473 median:0.335911\n",
      "Sensitive TP/(TP+FN) : 0.660000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       100\n",
      "           1       0.66      0.66      0.66       100\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       200\n",
      "   macro avg       0.66      0.66      0.66       200\n",
      "weighted avg       0.66      0.66      0.66       200\n",
      "\n",
      "y_hat mean:0.332910 median:0.222005\n",
      "Sensitive TP/(TP+FN) : 0.710000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71       100\n",
      "           1       0.71      0.71      0.71       100\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       200\n",
      "   macro avg       0.71      0.71      0.71       200\n",
      "weighted avg       0.71      0.71      0.71       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.316107 median:0.193617\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n",
      "y_hat mean:0.348547 median:0.282025\n",
      "Sensitive TP/(TP+FN) : 0.630000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63       100\n",
      "           1       0.63      0.63      0.63       100\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       200\n",
      "   macro avg       0.63      0.63      0.63       200\n",
      "weighted avg       0.63      0.63      0.63       200\n",
      "\n",
      "y_hat mean:0.301066 median:0.168492\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n",
      "y_hat mean:0.347572 median:0.246113\n",
      "Sensitive TP/(TP+FN) : 0.590000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.59      0.59       100\n",
      "           1       0.59      0.59      0.59       100\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       200\n",
      "   macro avg       0.59      0.59      0.59       200\n",
      "weighted avg       0.59      0.59      0.59       200\n",
      "\n",
      "y_hat mean:0.354132 median:0.299565\n",
      "Sensitive TP/(TP+FN) : 0.680000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       100\n",
      "           1       0.68      0.68      0.68       100\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       200\n",
      "   macro avg       0.68      0.68      0.68       200\n",
      "weighted avg       0.68      0.68      0.68       200\n",
      "\n",
      "y_hat mean:0.329756 median:0.230243\n",
      "Sensitive TP/(TP+FN) : 0.660000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       100\n",
      "           1       0.66      0.66      0.66       100\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       200\n",
      "   macro avg       0.66      0.66      0.66       200\n",
      "weighted avg       0.66      0.66      0.66       200\n",
      "\n",
      "y_hat mean:0.328570 median:0.228274\n",
      "Sensitive TP/(TP+FN) : 0.640000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64       100\n",
      "           1       0.64      0.64      0.64       100\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       200\n",
      "   macro avg       0.64      0.64      0.64       200\n",
      "weighted avg       0.64      0.64      0.64       200\n",
      "\n",
      "y_hat mean:0.354449 median:0.280845\n",
      "Sensitive TP/(TP+FN) : 0.640000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64       100\n",
      "           1       0.64      0.64      0.64       100\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       200\n",
      "   macro avg       0.64      0.64      0.64       200\n",
      "weighted avg       0.64      0.64      0.64       200\n",
      "\n",
      "y_hat mean:0.340479 median:0.270329\n",
      "Sensitive TP/(TP+FN) : 0.680000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       100\n",
      "           1       0.68      0.68      0.68       100\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       200\n",
      "   macro avg       0.68      0.68      0.68       200\n",
      "weighted avg       0.68      0.68      0.68       200\n",
      "\n",
      "y_hat mean:0.351804 median:0.317782\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n",
      "y_hat mean:0.342051 median:0.296641\n",
      "Sensitive TP/(TP+FN) : 0.660000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       100\n",
      "           1       0.66      0.66      0.66       100\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       200\n",
      "   macro avg       0.66      0.66      0.66       200\n",
      "weighted avg       0.66      0.66      0.66       200\n",
      "\n",
      "y_hat mean:0.345090 median:0.258489\n",
      "Sensitive TP/(TP+FN) : 0.610000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.61      0.61       100\n",
      "           1       0.61      0.61      0.61       100\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       200\n",
      "   macro avg       0.61      0.61      0.61       200\n",
      "weighted avg       0.61      0.61      0.61       200\n",
      "\n",
      "y_hat mean:0.327673 median:0.207891\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n",
      "y_hat mean:0.334974 median:0.246918\n",
      "Sensitive TP/(TP+FN) : 0.700000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       100\n",
      "           1       0.70      0.70      0.70       100\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       200\n",
      "   macro avg       0.70      0.70      0.70       200\n",
      "weighted avg       0.70      0.70      0.70       200\n",
      "\n",
      "y_hat mean:0.326729 median:0.201637\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n",
      "y_hat mean:0.311526 median:0.185692\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.360884 median:0.338662\n",
      "Sensitive TP/(TP+FN) : 0.620000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62       100\n",
      "           1       0.62      0.62      0.62       100\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       200\n",
      "   macro avg       0.62      0.62      0.62       200\n",
      "weighted avg       0.62      0.62      0.62       200\n",
      "\n",
      "y_hat mean:0.306756 median:0.146400\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n",
      "y_hat mean:0.318314 median:0.176954\n",
      "Sensitive TP/(TP+FN) : 0.700000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       100\n",
      "           1       0.70      0.70      0.70       100\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       200\n",
      "   macro avg       0.70      0.70      0.70       200\n",
      "weighted avg       0.70      0.70      0.70       200\n",
      "\n",
      "y_hat mean:0.306625 median:0.133410\n",
      "Sensitive TP/(TP+FN) : 0.700000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       100\n",
      "           1       0.70      0.70      0.70       100\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       200\n",
      "   macro avg       0.70      0.70      0.70       200\n",
      "weighted avg       0.70      0.70      0.70       200\n",
      "\n",
      "y_hat mean:0.331532 median:0.196801\n",
      "Sensitive TP/(TP+FN) : 0.700000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       100\n",
      "           1       0.70      0.70      0.70       100\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       200\n",
      "   macro avg       0.70      0.70      0.70       200\n",
      "weighted avg       0.70      0.70      0.70       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.341314 median:0.294830\n",
      "Sensitive TP/(TP+FN) : 0.660000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       100\n",
      "           1       0.66      0.66      0.66       100\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       200\n",
      "   macro avg       0.66      0.66      0.66       200\n",
      "weighted avg       0.66      0.66      0.66       200\n",
      "\n",
      "y_hat mean:0.321571 median:0.247313\n",
      "Sensitive TP/(TP+FN) : 0.680000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       100\n",
      "           1       0.68      0.68      0.68       100\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       200\n",
      "   macro avg       0.68      0.68      0.68       200\n",
      "weighted avg       0.68      0.68      0.68       200\n",
      "\n",
      "y_hat mean:0.337430 median:0.219326\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.345678 median:0.270265\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.340448 median:0.268914\n",
      "Sensitive TP/(TP+FN) : 0.640000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64       100\n",
      "           1       0.64      0.64      0.64       100\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       200\n",
      "   macro avg       0.64      0.64      0.64       200\n",
      "weighted avg       0.64      0.64      0.64       200\n",
      "\n",
      "y_hat mean:0.326852 median:0.218055\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.326333 median:0.163017\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.326331 median:0.200800\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n",
      "y_hat mean:0.344737 median:0.308317\n",
      "Sensitive TP/(TP+FN) : 0.610000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.61      0.61       100\n",
      "           1       0.61      0.61      0.61       100\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       200\n",
      "   macro avg       0.61      0.61      0.61       200\n",
      "weighted avg       0.61      0.61      0.61       200\n",
      "\n",
      "y_hat mean:0.356805 median:0.274494\n",
      "Sensitive TP/(TP+FN) : 0.620000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62       100\n",
      "           1       0.62      0.62      0.62       100\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       200\n",
      "   macro avg       0.62      0.62      0.62       200\n",
      "weighted avg       0.62      0.62      0.62       200\n",
      "\n",
      "y_hat mean:0.325699 median:0.249638\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.361727 median:0.289683\n",
      "Sensitive TP/(TP+FN) : 0.660000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       100\n",
      "           1       0.66      0.66      0.66       100\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       200\n",
      "   macro avg       0.66      0.66      0.66       200\n",
      "weighted avg       0.66      0.66      0.66       200\n",
      "\n",
      "y_hat mean:0.322486 median:0.172724\n",
      "Sensitive TP/(TP+FN) : 0.700000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       100\n",
      "           1       0.70      0.70      0.70       100\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       200\n",
      "   macro avg       0.70      0.70      0.70       200\n",
      "weighted avg       0.70      0.70      0.70       200\n",
      "\n",
      "y_hat mean:0.297295 median:0.161637\n",
      "Sensitive TP/(TP+FN) : 0.720000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72       100\n",
      "           1       0.72      0.72      0.72       100\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       200\n",
      "   macro avg       0.72      0.72      0.72       200\n",
      "weighted avg       0.72      0.72      0.72       200\n",
      "\n",
      "y_hat mean:0.347549 median:0.274612\n",
      "Sensitive TP/(TP+FN) : 0.690000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       100\n",
      "           1       0.69      0.69      0.69       100\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       200\n",
      "   macro avg       0.69      0.69      0.69       200\n",
      "weighted avg       0.69      0.69      0.69       200\n",
      "\n",
      "y_hat mean:0.358966 median:0.317592\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.330730 median:0.227461\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n",
      "y_hat mean:0.337306 median:0.219617\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.347861 median:0.256546\n",
      "Sensitive TP/(TP+FN) : 0.680000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       100\n",
      "           1       0.68      0.68      0.68       100\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       200\n",
      "   macro avg       0.68      0.68      0.68       200\n",
      "weighted avg       0.68      0.68      0.68       200\n",
      "\n",
      "y_hat mean:0.308736 median:0.209345\n",
      "Sensitive TP/(TP+FN) : 0.670000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       100\n",
      "           1       0.67      0.67      0.67       100\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.67      0.67      0.67       200\n",
      "weighted avg       0.67      0.67      0.67       200\n",
      "\n",
      "y_hat mean:0.315419 median:0.181304\n",
      "Sensitive TP/(TP+FN) : 0.650000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       100\n",
      "           1       0.65      0.65      0.65       100\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat mean:0.331471 median:0.245994\n",
      "Sensitive TP/(TP+FN) : 0.620000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62       100\n",
      "           1       0.62      0.62      0.62       100\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       200\n",
      "   macro avg       0.62      0.62      0.62       200\n",
      "weighted avg       0.62      0.62      0.62       200\n",
      "\n",
      "y_hat mean:0.365114 median:0.342091\n",
      "Sensitive TP/(TP+FN) : 0.660000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       100\n",
      "           1       0.66      0.66      0.66       100\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       200\n",
      "   macro avg       0.66      0.66      0.66       200\n",
      "weighted avg       0.66      0.66      0.66       200\n",
      "\n",
      "y_hat mean:0.326244 median:0.156941\n",
      "Sensitive TP/(TP+FN) : 0.750000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       100\n",
      "           1       0.75      0.75      0.75       100\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       200\n",
      "   macro avg       0.75      0.75      0.75       200\n",
      "weighted avg       0.75      0.75      0.75       200\n",
      "\n",
      "y_hat mean:0.329777 median:0.251171\n",
      "Sensitive TP/(TP+FN) : 0.680000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       100\n",
      "           1       0.68      0.68      0.68       100\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       200\n",
      "   macro avg       0.68      0.68      0.68       200\n",
      "weighted avg       0.68      0.68      0.68       200\n",
      "\n",
      "(10200, 1)\n",
      "(10200, 1)\n",
      "y_hat mean:0.500000 median:0.500000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from plotLayer import *\n",
    "from preprocess import *\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy import sparse as sp\n",
    "from skimage.measure import block_reduce\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report, precision_recall_curve\n",
    "\n",
    "from model import *\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "def Classify_Rate(y, y_hat):\n",
    "    # True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
    "    TP = np.sum(np.logical_and(y_hat == 1, y == 1))\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(y_hat == 0, y == 0))\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(y_hat == 1, y == 0))\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(y_hat == 0, y == 1))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def Predict(model, x, y_threshold=None):\n",
    "    y_hat = model.predict(x)\n",
    "\n",
    "    if y_threshold :\n",
    "        y_hat[y_hat < y_threshold] = 0\n",
    "        y_hat[y_hat >= y_threshold] = 1\n",
    "    return y_hat\n",
    "\n",
    "def PlotROC(y, y_hat):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(2):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y, y_hat)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    print (\"roc_auc_score:%f\" %roc_auc_score(y, y_hat))\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[1], tpr[1])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.show()\n",
    "\n",
    "def PreprocessData(datapath, width=256, channel=6):\n",
    "    ratio = 1024 // width\n",
    "    with open(datapath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    x = np.heaviside(np.array([map(lambda x: block_reduce(x.toarray(), block_size=(ratio,ratio), func=np.max), d.hL) for d in data]), 0)\n",
    "    x = np.swapaxes(x, 1, 3)\n",
    "    y = np.array([d.label for d in data])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    width = 256\n",
    "    channel = 6\n",
    "    #classify_weights_path = \"Classify_epoch_50_batch_4.hdf5\" the below model is copied one directory above\n",
    "    classify_weights_path = \"Classify_epoch_2_batch_5.hdf5\"\n",
    "\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "    classify_model = Encoder_Classify(input_size=(width,width,6), batch_normal=True)\n",
    "    classify_model.load_weights(classify_weights_path)\n",
    "\n",
    "    d0_path = \"../Data/1stDataset/d0*\"\n",
    "    false_path = \"../Data/1stDataset/false*\"\n",
    "    d0 = np.sort(glob.glob(d0_path))\n",
    "    f0 = np.sort(glob.glob(false_path))\n",
    "\n",
    "    y_total = []\n",
    "    y_hat_total = []\n",
    "    #proba is a keyword used in sklearn for probabilities\n",
    "    proba = []\n",
    "#     301\n",
    "    for i in range(500,551):\n",
    "        x1, y1 = PreprocessData(d0[i], width=width, channel=channel)\n",
    "        x2, y2 = PreprocessData(f0[i], width=width, channel=channel)\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "        y = np.concatenate((y1, y2), axis=0)\n",
    "\n",
    "        y_hat = Predict(classify_model, x)\n",
    "#         print(y_hat)\n",
    "        y_total += [y]\n",
    "        y_hat_total += [y_hat]\n",
    "        proba += [y_hat.copy()]\n",
    "# print(proba)-------You should be very careful in numpy as numpy arrays as get cop\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html\n",
    "# Check the section sub arrays as no copy views\n",
    "        print(\"y_hat mean:%f median:%f\" %(np.mean(y_hat), np.median(y_hat)))\n",
    "        y_hat[y_hat < np.median(y_hat)] = 0\n",
    "        y_hat[y_hat >= np.median(y_hat)] = 1\n",
    "\n",
    "        TP, TN, FP, FN = Classify_Rate(y, y_hat)\n",
    "        print(\"Sensitive TP/(TP+FN) : %f\" %(TP / (TP + FN)))\n",
    "        print(classification_report(y, y_hat))\n",
    "\n",
    "    y = np.concatenate(y_total)\n",
    "    y_hat = np.concatenate(y_hat_total)\n",
    "    proba = np.concatenate(proba)\n",
    "    a=np.hstack((y,proba))\n",
    "    np.savetxt(\"newmodel_y_y_hat.txt\",a,delimiter=',')\n",
    "    print(y.shape)\n",
    "    print(y_hat.shape)\n",
    "    print(\"y_hat mean:%f median:%f\" %(np.mean(y_hat), np.median(y_hat)))\n",
    "    fp,tp,tr=roc_curve(y,proba)\n",
    "    p,r,tr1=precision_recall_curve(y,proba)\n",
    "    plt.figure(1)\n",
    "    plt.plot(fp,tp,color='darkorange',lw=2,label='ROC curve(area = %0.3f)'%auc(fp,tp))\n",
    "    plt.xlabel('False positive Rate')\n",
    "    plt.ylabel('True positive Rate')\n",
    "    legend = plt.legend(fontsize = 'x-large')\n",
    "    plt.savefig('ROC_curve_newmodel.png')\n",
    "    plt.show()\n",
    "    plt.figure(2)\n",
    "    plt.plot(r,p,color='darkorange', label='Precision recall curve') \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    legend=plt.legend(fontsize='x-large')\n",
    "    plt.savefig('Precision_Recall_newmodel.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve area with the new model trained on 10 samples= 0.658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive TP/(TP+FN) : 0.660980\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66      5100\n",
      "           1       0.66      0.66      0.66      5100\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     10200\n",
      "   macro avg       0.66      0.66      0.66     10200\n",
      "weighted avg       0.66      0.66      0.66     10200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    y_hat[y_hat < np.median(y_hat)] = 0\n",
    "    y_hat[y_hat >= np.median(y_hat)] = 1\n",
    "    TP, TN, FP, FN = Classify_Rate(y, y_hat)\n",
    "    print(\"Sensitive TP/(TP+FN) : %f\" %(TP / (TP + FN)))\n",
    "    print(classification_report(y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(proba)\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [0.8, 0.9, 0.85, 0]).ravel()\n",
    "# What ever I have presented yesterday those contain thresholded values. \n",
    "# from sklearn.metrics import classification_report\n",
    "# y_true=np.array([0,1,1,1,0,0,1])\n",
    "# y_pred=np.array([0.3,0.9,0.8,0.3,0.2,0.3,0.8])\n",
    "# print(classification_report(y_true, y_pred))\n",
    "#Classification report as well as the confusion_matrix works only for a particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
